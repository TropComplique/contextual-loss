{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from contextual_loss import ContextualLoss\n",
    "from other_losses import TotalVariationLoss\n",
    "from extractor import Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = Image.open('dog.jpg').resize((300, 200))\n",
    "style = Image.open('The_Starry_Night.jpg').resize((600, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    x = np.array(x)\n",
    "    x = torch.FloatTensor(x)\n",
    "    # convert to the NCHW format and the [0, 1] range\n",
    "    return x.permute(2, 0, 1).unsqueeze(0)/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use contextual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "\n",
    "    def __init__(self, content, style):\n",
    "        super(Loss, self).__init__()\n",
    "\n",
    "        # images\n",
    "        c = to_tensor(content)\n",
    "        s = to_tensor(style)\n",
    "        mean, std = 0.5, 1e-3\n",
    "        self.x = nn.Parameter(data=mean + std * torch.randn(c.size()), requires_grad=True)\n",
    "\n",
    "        # features\n",
    "        vgg = Extractor()\n",
    "        cf = vgg(c)\n",
    "        sf = vgg(s)\n",
    "\n",
    "        # names of features to use\n",
    "        content_layers = ['conv4_1', 'conv5_1']\n",
    "        style_layers = ['conv4_1', 'conv5_1']\n",
    "\n",
    "        # create losses\n",
    "        self.content = nn.ModuleDict({\n",
    "            n: ContextualLoss(cf[n], size=5, stride=2, h=0.1) \n",
    "            for n in content_layers\n",
    "        })\n",
    "        self.style = nn.ModuleDict({\n",
    "            n: ContextualLoss(sf[n], size=5, stride=2, h=0.2) \n",
    "            for n in style_layers\n",
    "        })\n",
    "        self.tv = TotalVariationLoss()\n",
    "        self.vgg = vgg\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "        f = self.vgg(self.x)\n",
    "        content_loss = torch.tensor(0.0, device=self.x.device)\n",
    "        style_loss = torch.tensor(0.0, device=self.x.device)\n",
    "        tv_loss = self.tv(self.x)\n",
    "            \n",
    "        for n, m in self.content.items():\n",
    "            content_loss += m(f[n])\n",
    "            \n",
    "        for n, m in self.style.items():\n",
    "            style_loss += m(f[n])\n",
    "    \n",
    "        return content_loss, style_loss, tv_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "objective = Loss(content, style).to(DEVICE)\n",
    "params = filter(lambda x: x.requires_grad, objective.parameters())\n",
    "\n",
    "NUM_STEPS = 2000\n",
    "optimizer = optim.Adam(params, lr=10.0)\n",
    "\n",
    "text = 'total:{0:.2f},content:{1:.3f},style:{2:.6f},tv:{3:.4f}'\n",
    "for i in range(NUM_STEPS):\n",
    "    \n",
    "    objective.x.data.clamp_(0, 1)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    content_loss, style_loss, tv_loss = objective()\n",
    "    total_loss = 100 * content_loss + style_loss + 10000 * tv_loss\n",
    "    total_loss.backward()\n",
    "\n",
    "    print(text.format(total_loss.item(), content_loss.item(), style_loss.item(), tv_loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 255 * objective.x.clamp(0, 1).detach().permute(0, 2, 3, 1)[0].cpu().numpy()\n",
    "Image.fromarray(result.astype('uint8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
